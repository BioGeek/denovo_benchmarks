name: AIchor Benchmark

# Only runs on manual trigger (workflow_dispatch)
# Does NOT run on push events - must be triggered manually via UI or CLI
on:
  workflow_dispatch:
    inputs:
      algorithms:
        description: 'Algorithms to benchmark (comma-separated or "all")'
        required: true
        default: 'instanovo'
        type: string
      dataset:
        description: 'Dataset path'
        required: false
        default: '/mnt/denovo_benchmarks/peak/mgf'
        type: string
      dataset_name:
        description: 'Dataset name for results'
        required: true
        default: '9_species_human'
        type: string
      experiment_ids:
        description: 'Experiment IDs to monitor (comma-separated, leave empty to monitor from submit_benchmark job)'
        required: false
        default: ''
        type: string
      monitor_only:
        description: 'Only monitor experiments (skip submission)'
        required: false
        default: false
        type: boolean
      force:
        description: 'Force resubmission: remove existing (algorithm, dataset) entries and resubmit'
        required: false
        default: false
        type: boolean
      gather_results_only:
        description: 'Only gather results (skip submission and monitoring)'
        required: false
        default: false
        type: boolean

jobs:
  submit_benchmark:
    runs-on: ${{ github.event.repository.private && 'instadeep-ci-4' || 'ubuntu-latest' }}
    container: ${{ github.event.repository.private && fromJSON('{"image":"ubuntu:22.04"}') || fromJSON('null') }}
    if: ${{ github.event.inputs.monitor_only != 'true' && github.event.inputs.gather_results_only != 'true' }}
    permissions:
      contents: write
      id-token: write
    
    steps:
      - name: Install system dependencies
        if: github.event.repository.private
        run: |
          apt-get update
          apt-get install -y ca-certificates git
      
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
      
      - name: Print workflow inputs
        run: |
          echo "Algorithms: ${{ github.event.inputs.algorithms }}"
          echo "Dataset: ${{ github.event.inputs.dataset }}"
          echo "Dataset name: ${{ github.event.inputs.dataset_name }}"
          echo "Repository private: ${{ github.event.repository.private }}"
          echo "Runner: ${{ runner.os }}"
      
      - name: Test environment
        run: |
          echo "Current directory: $(pwd)"
          echo "User: $(whoami)"
          echo "OS Info: $(uname -a)"
          ls -la
      
      - name: Set up uv
        uses: astral-sh/setup-uv@v6
      
      - name: Install Python
        run: uv python install 3.11
      
      - name: Install dependencies
        run: |
          uv sync --all-extras
      
      - name: Authenticate with AIchor
        run: uv run aichor auth key --apikey ${{ secrets.AICHOR_API_KEY }}
        env:
          AICHOR_API_KEY: ${{ secrets.AICHOR_API_KEY }}
      
      - name: Submit AIchor experiments
        id: submit
        run: |
          FORCE_FLAG=""
          if [ "${{ github.event.inputs.force }}" = "true" ] || [ "${{ github.event.inputs.force }}" = true ]; then
            FORCE_FLAG="--force"
          fi
          
          echo "Submitting experiments with flags:"
          echo "  Algorithms: ${{ github.event.inputs.algorithms }}"
          echo "  Dataset name: ${{ github.event.inputs.dataset_name }}"
          echo "  Dataset path: ${{ github.event.inputs.dataset }}"
          echo "  Force flag: ${FORCE_FLAG:-none}"
          
          set +e  # Don't exit on error, we'll handle it
          if [ -n "$FORCE_FLAG" ]; then
            OUTPUT=$(uv run python scripts/submit_benchmark_aichor.py \
              --algorithms "${{ github.event.inputs.algorithms }}" \
              --dataset-name "${{ github.event.inputs.dataset_name }}" \
              --dataset-path "${{ github.event.inputs.dataset }}" \
              --output experiment_ids.json \
              $FORCE_FLAG 2>&1)
          else
            OUTPUT=$(uv run python scripts/submit_benchmark_aichor.py \
              --algorithms "${{ github.event.inputs.algorithms }}" \
              --dataset-name "${{ github.event.inputs.dataset_name }}" \
              --dataset-path "${{ github.event.inputs.dataset }}" \
              --output experiment_ids.json 2>&1)
          fi
          EXIT_CODE=$?
          set -e  # Re-enable exit on error
          
          echo "$OUTPUT"
          
          if [ $EXIT_CODE -ne 0 ]; then
            echo "‚ùå Error: submit_benchmark_aichor.py failed with exit code $EXIT_CODE"
            echo "Output was:"
            echo "$OUTPUT"
            exit $EXIT_CODE
          fi
          
          # Extract experiment IDs from output
          EXPERIMENT_IDS=$(echo "$OUTPUT" | grep "^EXPERIMENT_IDS=" | cut -d'=' -f2 || echo "")
          if [ -n "$EXPERIMENT_IDS" ]; then
            echo "experiment_ids=$EXPERIMENT_IDS" >> $GITHUB_OUTPUT
            echo "Found experiment IDs: $EXPERIMENT_IDS"
          else
            echo "No new experiment IDs found in output"
            echo "experiment_ids=" >> $GITHUB_OUTPUT
          fi
      
      - name: Display experiment IDs
        run: |
          echo "Submitted experiments:"
          cat experiment_ids.json
          echo ""
          echo "Experiment IDs to monitor: ${{ steps.submit.outputs.experiment_ids }}"
      
      - name: Commit experiment_ids.json
        if: always()
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          
          if [ -f experiment_ids.json ]; then
            # Check if there are changes to commit
            git add experiment_ids.json
            if git diff --cached --quiet; then
              echo "No changes to experiment_ids.json"
            else
              COMMIT_MSG="Update experiment_ids.json: ${{ github.event.inputs.algorithms }} on ${{ github.event.inputs.dataset_name }} [skip ci]"
              git commit -m "$COMMIT_MSG"
              git push origin HEAD:${{ github.ref }} || echo "Failed to push (may need manual push or permissions)"
            fi
          else
            echo "experiment_ids.json not found, skipping commit"
          fi
      
      - name: Output experiment IDs for monitoring
        if: always()
        run: |
          if [ -n "${{ steps.submit.outputs.experiment_ids }}" ]; then
            echo "experiment_ids=${{ steps.submit.outputs.experiment_ids }}" >> $GITHUB_OUTPUT
            echo "Job-level output: experiment_ids=${{ steps.submit.outputs.experiment_ids }}"
          else
            echo "experiment_ids=" >> $GITHUB_OUTPUT
            echo "No new experiment IDs to output (may monitor existing from experiment_ids.json)"
          fi
  
  monitor_experiments:
    runs-on: ${{ github.event.repository.private && 'instadeep-ci-4' || 'ubuntu-latest' }}
    container: ${{ github.event.repository.private && fromJSON('{"image":"ubuntu:22.04"}') || fromJSON('null') }}
    needs: submit_benchmark
    if: ${{ github.event.inputs.gather_results_only != 'true' && (github.event.inputs.monitor_only == 'true' || (needs.submit_benchmark.result != 'skipped' && (needs.submit_benchmark.result == 'success' || needs.submit_benchmark.result == 'failure'))) }}
    permissions:
      contents: read
      id-token: write
    
    steps:
      - name: Install system dependencies
        if: github.event.repository.private
        run: |
          apt-get update
          apt-get install -y ca-certificates git
      
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up uv
        uses: astral-sh/setup-uv@v6
      
      - name: Install Python
        run: uv python install 3.11
      
      - name: Install dependencies
        run: |
          uv sync --all-extras
      
      - name: Authenticate with AIchor
        run: uv run aichor auth key --apikey ${{ secrets.AICHOR_API_KEY }}
        env:
          AICHOR_API_KEY: ${{ secrets.AICHOR_API_KEY }}
      
      - name: Determine experiment IDs to monitor
        id: determine_ids
        run: |
          echo "Debug: Checking experiment ID sources..."
          echo "  Manual input (experiment_ids): '${{ github.event.inputs.experiment_ids }}'"
          echo "  Monitor only: '${{ github.event.inputs.monitor_only }}'"
          echo "  Submit benchmark result: '${{ needs.submit_benchmark.result }}'"
          echo "  Submit benchmark outputs.experiment_ids: '${{ needs.submit_benchmark.outputs.experiment_ids }}'"
          
          # Use manual input if provided
          MANUAL_IDS="${{ github.event.inputs.experiment_ids }}"
          if [ -n "$MANUAL_IDS" ]; then
            EXPERIMENT_IDS="$MANUAL_IDS"
            echo "‚úÖ Using manually provided experiment IDs: $EXPERIMENT_IDS"
          # Use from submit_benchmark job if available (and not skipped)
          elif [ "${{ github.event.inputs.monitor_only }}" != "true" ] && [ "${{ needs.submit_benchmark.result }}" != "skipped" ]; then
            # Check if the output exists (even if empty, it means the step ran)
            SUBMIT_EXP_IDS="${{ needs.submit_benchmark.outputs.experiment_ids }}"
            # Check if output was actually set (not just empty string from default)
            # We need to check if the job output exists, which we can infer from the result
            if [ "${{ needs.submit_benchmark.result }}" = "success" ] || [ "${{ needs.submit_benchmark.result }}" = "failure" ]; then
              if [ -n "$SUBMIT_EXP_IDS" ]; then
                EXPERIMENT_IDS="$SUBMIT_EXP_IDS"
                echo "‚úÖ Using experiment IDs from submit_benchmark job: $EXPERIMENT_IDS"
              else
                echo "‚ö†Ô∏è  submit_benchmark completed but no experiment IDs in output"
                echo "   This means no new experiments were submitted (all were skipped)"
                echo "   Will monitor all experiments from experiment_ids.json"
                EXPERIMENT_IDS=""
              fi
            else
              echo "‚ö†Ô∏è  submit_benchmark result is '${{ needs.submit_benchmark.result }}', will monitor all"
              EXPERIMENT_IDS=""
            fi
          else
            # Fallback: read from experiment_ids.json if it exists
            if [ -f experiment_ids.json ]; then
              echo "‚ö†Ô∏è  No specific experiment IDs provided, will monitor all experiments in experiment_ids.json"
              EXPERIMENT_IDS=""
            else
              echo "‚ö†Ô∏è  Warning: No experiment IDs found and experiment_ids.json does not exist"
              echo "This might happen if submit_benchmark was skipped or no experiments were submitted"
              echo "Will attempt to monitor anyway (monitor script will handle empty file)"
              EXPERIMENT_IDS=""
            fi
          fi
          echo "Final EXPERIMENT_IDS: '$EXPERIMENT_IDS'"
          echo "experiment_ids=$EXPERIMENT_IDS" >> $GITHUB_OUTPUT
      
      - name: Monitor experiments
        run: |
          EXPERIMENT_IDS="${{ steps.determine_ids.outputs.experiment_ids }}"
          if [ -n "$EXPERIMENT_IDS" ]; then
            echo "Monitoring experiments: $EXPERIMENT_IDS"
            uv run python scripts/monitor_experiment.py "$EXPERIMENT_IDS" \
              --experiment-ids-file experiment_ids.json \
              --poll-interval 32
          else
            echo "Monitoring all experiments from experiment_ids.json"
            uv run python scripts/monitor_experiment.py \
              --experiment-ids-file experiment_ids.json \
              --poll-interval 32
          fi
        env:
          AICHOR_API_KEY: ${{ secrets.AICHOR_API_KEY }}
  
  gather_results:
    runs-on: ${{ github.event.repository.private && 'instadeep-ci-4' || 'ubuntu-latest' }}
    container: ${{ github.event.repository.private && fromJSON('{"image":"ubuntu:22.04"}') || fromJSON('null') }}
    needs: monitor_experiments
    if: ${{ github.event.inputs.gather_results_only == 'true' || needs.monitor_experiments.result == 'success' || needs.monitor_experiments.result == 'failure' || needs.monitor_experiments.result == 'skipped' }}
    permissions:
      contents: read
      id-token: write
    
    steps:
      - name: Install system dependencies
        if: github.event.repository.private
        run: |
          apt-get update
          apt-get install -y ca-certificates git
      
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up uv
        uses: astral-sh/setup-uv@v6
      
      - name: Install Python
        run: uv python install 3.11
      
      - name: Install dependencies
        run: |
          uv sync --all-extras
      
      - name: Inspect results directory (debug)
        run: |
          RESULTS_DIR="/mnt/denovo_benchmarks/results_aichor"
          echo "üìÇ Inspecting results directory: $RESULTS_DIR"
          if [ -d "$RESULTS_DIR" ]; then
            echo "Directory exists"
            echo ""
            echo "Contents:"
            ls -lah "$RESULTS_DIR" || echo "Could not list directory (may need permissions)"
            echo ""
            echo "Subdirectories:"
            find "$RESULTS_DIR" -maxdepth 2 -type d 2>/dev/null | head -20 || echo "Could not traverse directory"
            echo ""
            echo "CSV files:"
            find "$RESULTS_DIR" -name "*_output.csv" -type f 2>/dev/null | head -20 || echo "No CSV files found or could not access"
          else
            echo "‚ö†Ô∏è  Directory does not exist yet (will be created by gather_results_aichor.py)"
            echo "Creating directory structure..."
            mkdir -p "$RESULTS_DIR" || echo "Could not create directory (may need permissions)"
          fi
      
      - name: Authenticate with AIchor
        run: uv run aichor auth key --apikey ${{ secrets.AICHOR_API_KEY }}
        env:
          AICHOR_API_KEY: ${{ secrets.AICHOR_API_KEY }}
      
      - name: Gather results from AIchor experiments
        run: |
          FORCE_FLAG=""
          if [ "${{ github.event.inputs.force }}" = "true" ] || [ "${{ github.event.inputs.force }}" = true ]; then
            FORCE_FLAG="--force"
          fi
          
          echo "Gathering results with parameters:"
          echo "  Dataset name: ${{ github.event.inputs.dataset_name }}"
          echo "  Force: ${FORCE_FLAG:-none}"
          echo "  Output directory: /mnt/denovo_benchmarks/results_aichor"
          
          # Run gather_results_aichor.py with --output-dir pointing to the desired location
          uv run python scripts/gather_results_aichor.py \
            --experiment-ids experiment_ids.json \
            --dataset-name "${{ github.event.inputs.dataset_name }}" \
            --output-dir /mnt/denovo_benchmarks/results_aichor \
            $FORCE_FLAG
          
          echo ""
          echo "‚úÖ Results gathering completed"
          echo "Results saved to: /mnt/denovo_benchmarks/results_aichor/${{ github.event.inputs.dataset_name }}/"
        env:
          AICHOR_API_KEY: ${{ secrets.AICHOR_API_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          S3_ENDPOINT: ${{ secrets.S3_ENDPOINT }}
      
      - name: Verify results were downloaded
        run: |
          RESULTS_DIR="/mnt/denovo_benchmarks/results_aichor/${{ github.event.inputs.dataset_name }}"
          echo "üìä Verifying results in: $RESULTS_DIR"
          if [ -d "$RESULTS_DIR" ]; then
            CSV_FILES=$(find "$RESULTS_DIR" -name "*_output.csv" -type f 2>/dev/null | wc -l)
            echo "Found $CSV_FILES CSV file(s)"
            if [ "$CSV_FILES" -gt 0 ]; then
              echo ""
              echo "Files:"
              find "$RESULTS_DIR" -name "*_output.csv" -type f 2>/dev/null
            fi
          else
            echo "‚ö†Ô∏è  Results directory not found: $RESULTS_DIR"
          fi
    